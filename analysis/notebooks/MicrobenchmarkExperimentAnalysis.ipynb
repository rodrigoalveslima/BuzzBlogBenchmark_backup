{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47a99037",
   "metadata": {},
   "source": [
    "# Microbenchmark Experiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061eb471",
   "metadata": {},
   "source": [
    "## Notebook Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729cf73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## GENERAL\n",
    "# Experiment directory path\n",
    "EXPERIMENT_DIRPATH = \"BuzzBlogBenchmark_%Y-%m-%d-%H-%M-%S\"\n",
    "\n",
    "########## EXECUTION LOGS\n",
    "# Function to aggregate PIT data\n",
    "PIT_AGGREGATE_FUNC = \"mean\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1022da8a",
   "metadata": {},
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3663d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import warnings\n",
    "import yaml\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-workstation",
   "metadata": {},
   "source": [
    "## Experiment Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-contest",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## UTILITIES\n",
    "def load_experiment_metadata():\n",
    "    \"Return a YAML object with experiment metadata.\"\n",
    "    with open(os.path.join(EXPERIMENT_DIRPATH, \"metadata.yml\")) as metadata_file:\n",
    "        return yaml.load(metadata_file, Loader=yaml.Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-hepatitis",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yaml.dump(load_experiment_metadata(), default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159875d0",
   "metadata": {},
   "source": [
    "## Execution Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8819a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## UTILITIES\n",
    "def get_microbenchmark_tarball_path():\n",
    "    \"Return the path to the microbenchmark tarball.\"\n",
    "    for node_hostname in os.listdir(os.path.join(EXPERIMENT_DIRPATH, \"logs\")):\n",
    "        for filename in os.listdir(os.path.join(EXPERIMENT_DIRPATH, \"logs\", node_hostname)):\n",
    "            if filename.endswith(\"_microbench.tar.gz\"):\n",
    "                return os.path.join(EXPERIMENT_DIRPATH, \"logs\", node_hostname, filename)\n",
    "\n",
    "def get_microbenchmarked_service():\n",
    "    \"Return the name of the microbenchmarked service\"\n",
    "    return get_microbenchmark_tarball_path().split(\"/\")[-1].split(\"_\")[0]\n",
    "            \n",
    "def list_microbenchmarks():\n",
    "    \"Return a list of microbenchmarks in the tarball.\"\n",
    "    tarball = tarfile.open(get_microbenchmark_tarball_path())\n",
    "    return [f.name.split(\".\")[-2].split(\"/\")[-1] for f in tarball if f.name.endswith(\".csv\")]\n",
    "\n",
    "def load_microbenchmark_logs(microbenchmark):\n",
    "    \"Return a DataFrame with execution logs of the specified microbenchmark.\"\n",
    "    tarball = tarfile.open(get_microbenchmark_tarball_path())\n",
    "    with tarball.extractfile(\"./microbench_%s/%s.csv\" % (get_microbenchmarked_service(), microbenchmark)) as microbenchmark_file:\n",
    "        df = pd.read_csv(microbenchmark_file)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45129a99",
   "metadata": {},
   "source": [
    "### Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdf6e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "microbenchmarks = list_microbenchmarks()\n",
    "fig = plt.figure(figsize=(16 * len(microbenchmarks), 32 * len(microbenchmarks)))\n",
    "for (i, microbenchmark) in enumerate(microbenchmarks):\n",
    "    df = load_microbenchmark_logs(microbenchmark)\n",
    "    df[\"window\"] = df.apply(lambda r: int(r[\"timestamp_milli\"] / 1000), axis=1)\n",
    "    df = df.groupby([\"window\"])[\"window\"].count()\n",
    "    df = df.reindex(range(0, int(df.index.max()) + 1), fill_value=0)\n",
    "    ax = fig.add_subplot(len(microbenchmarks), 1, i + 1)\n",
    "    ax.grid(alpha=0.75)\n",
    "    df.plot(ax=ax, kind=\"bar\", title=\"Throughput: %s microbenchmark\" % microbenchmark,\n",
    "            xlabel=\"Time (seconds)\", ylabel=\"Calls per second\",\n",
    "            color=\"blue\", grid=True, xticks=range(0, int(df.index.max()) + 1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af71c46",
   "metadata": {},
   "source": [
    "### Point-in-Time Execution Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75722c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "microbenchmarks = list_microbenchmarks()\n",
    "fig = plt.figure(figsize=(16 * len(microbenchmarks), 32 * len(microbenchmarks)))\n",
    "for (i, microbenchmark) in enumerate(microbenchmarks):\n",
    "    df = load_microbenchmark_logs(microbenchmark)\n",
    "    df[\"window\"] = df.apply(lambda r: int(r[\"timestamp_milli\"] / 1000), axis=1)\n",
    "    df = df.groupby([\"window\"])[\"exec_time_milli\"].agg(PIT_AGGREGATE_FUNC)\n",
    "    df = df.reindex(range(0, int(df.index.max()) + 1), fill_value=0)\n",
    "    ax = fig.add_subplot(len(microbenchmarks), 1, i + 1)\n",
    "    ax.grid(alpha=0.75)\n",
    "    df.plot(ax=ax, kind=\"bar\", title=\"PIT Execution Time: %s microbenchmark\" % microbenchmark,\n",
    "            xlabel=\"Time (seconds)\", ylabel=\"%s Execution Time (milliseconds)\" % PIT_AGGREGATE_FUNC,\n",
    "            color=\"purple\", grid=True, xticks=range(0, int(df.index.max()) + 1, 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c522a91f",
   "metadata": {},
   "source": [
    "### Execution Time Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0477ec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "microbenchmarks = list_microbenchmarks()\n",
    "fig = plt.figure(figsize=(16 * len(microbenchmarks), 32 * len(microbenchmarks)))\n",
    "for (i, microbenchmark) in enumerate(microbenchmarks):\n",
    "    df = load_microbenchmark_logs(microbenchmark)\n",
    "    df[\"exec_time_bin\"] = df.apply(lambda r: int(r[\"exec_time_milli\"]), axis=1)\n",
    "    ax = fig.add_subplot(len(microbenchmarks), 1, i + 1)\n",
    "    ax.grid(alpha=0.75)\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_xlim((0, df[\"exec_time_bin\"].max()))\n",
    "    df[\"exec_time_bin\"].plot(ax=ax, kind=\"hist\",\n",
    "                             title=\"Execution Time Distribution: %s microbenchmark\" % microbenchmark,\n",
    "                             xlabel=\"Execution Time (milliseconds)\", ylabel=\"Count\",\n",
    "                             bins=range(df[\"exec_time_bin\"].max()),\n",
    "                             grid=True, color=\"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-dispute",
   "metadata": {},
   "source": [
    "## Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-finding",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## UTILITIES\n",
    "def load_system_conf():\n",
    "    \"Return a YAML object with system configuration.\"\n",
    "    with open(os.path.join(EXPERIMENT_DIRPATH, \"conf\", \"system.yml\")) as system_conf_file:\n",
    "        return yaml.load(system_conf_file, Loader=yaml.Loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-retirement",
   "metadata": {},
   "source": [
    "### System Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-kuwait",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yaml.dump(load_system_conf()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
